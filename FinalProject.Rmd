---
title: "American Community Survery Project"
author: "Jonah Maroszek"
date: "3/24/2021"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(party)
library(knitr)
library(magrittr)
```
## Introduction
In this project I will use data for the state of Tennessee collected by ACS (American Community Survery) in order to estimate the mean of the variable INTP, which measures interest, dividends, and rental income. The initial data set contains 68,415 observations with 288 variables, though not all will be needed in predicting INTP. There are also a large number of missing values in this data set. I will be using multiple methods of imputation to correct for these deficiencies, as well as multiple methods to predict INTP.

```{r Data Import and Diagnosis, cache=TRUE, include=FALSE}
#will need to correct column types later on 
data = read_csv("psam_p47.csv", guess_max = 10000)
initial.dimensions = dim(data)

#finds variables where every value is the same to exclude from analysis
all.same = apply(data, 2, function(a) length(unique(a))==1)
cols.all.same = data[all.same]
```

## Data Cleaning
There are 8 variables in the data set that either have only one unique value, or they are a unique identifier, and thus not useful for prediction of INTP. These variables are: ADJINC, DIVISION, FANCP, REGION, RT, SERIALNO, SPORDER, ST.  

In addition to these variables, the variables PWGTP1-80 have been excluded, as these are sampling weights that are not needed for this analysis.  

Of the remaining variables, 43 of them have at least half of the values missing. The top ten variables that have the most missing values can be seen in the following table.
```{r Data Cleaning, echo=FALSE, cache=TRUE}
#First Phase of Data cleaning
vars.to.drop = c("ADJINC", "DIVISION", "FANCP", "REGION", "RT", "SERIALNO", "SPORDER", "ST")
clean.data.1 = data %>% 
  select(-all_of(vars.to.drop)) %>% #drops vars that have one unique value and unique identifiers not useful for prediction
  select(-num_range("PWGTP", 1:80)) #drop unneeded sampling weights

dim.clean.data.1 = dim(clean.data.1)

#find proportion of missing values for remaining variables
missing.vals.logi = apply(clean.data.1,2, is.na)
prop.missing.vals = apply(missing.vals.logi, 2, sum) / dim.clean.data.1[1]
large.prop.missing.vals = prop.missing.vals[prop.missing.vals > 0.5]
top.ten.missing.vals = sort(large.prop.missing.vals, decreasing = TRUE)[1:10]
kable(top.ten.missing.vals, col.names = c("Proportion Missing"), caption = "Variables and Proportion of Missing Values", digits = 3)
```

```{r Variable Type Correction, include=FALSE, cache = TRUE}
num.vars = c("PWGTP","INTP", "AGEP", "JWMNP", "OIP","JWRIP","PAP", "OIP", "RETP","SEMP", "SSIP", "SSP", "WKWN", "WKHP", "WAGP","PINCP", "PERNP","POVPIP")
varnames.clean = names(clean.data.1)
cat.vars = varnames.clean[!(varnames.clean %in% num.vars)] #all categorical variables

#this data set has correct column types 
clean.data.2 = clean.data.1 %>% 
  mutate(across(all_of(num.vars), as.double)) %>% #changes type to double 
  mutate(across(all_of(cat.vars), as.character)) #changes type to character 

write_csv(clean.data.2, "cleanData.csv")

```

```{r Subset creation for importance scoring}
#take a small, random sample from the cleaned data frame in order to decrease run time of guide importance scoring algorithm
dim.clean.data.2 = dim(clean.data.2)
subset.indices = sample(1:dim.clean.data.2[1], dim.clean.data.2[1]/4) #1/4 the size of original
subset = clean.data.2[subset.indices,]
write_csv(subset,"QuarterSubset.csv") 
```

```{r create guide description file for cleaned data set}
nvar = ncol(clean.data.2)
varnames = names(clean.data.2)
roles = rep("c", nvar) #default is categorical vars
num.vars = c("PWGTP","INTP", "AGEP", "JWMNP", "OIP","JWRIP","PAP", "OIP", "RETP","SEMP", "SSIP", "SSP", "WKWN", "WKHP", "WAGP","PINCP", "PERNP","POVPIP")
roles[varnames %in% num.vars] = "n" #numerical variables
roles[varnames ==  "INTP"] = "d" #dependent variable
roles[varnames %in% vars.to.drop] = "x"

#produce GUIDE Description File
write("cleanData.csv", file = "GUIDEdesc.txt")
write("NA",file="GUIDEdesc.txt",append=TRUE)
write("2",file="GUIDEdesc.txt",append=TRUE)
write.table(cbind(1:nvar,varnames,roles),file="GUIDEdesc.txt",
row.names=FALSE,col.names=FALSE,quote=FALSE,append=TRUE)
```
#to do 
exclude variables found to be unimportant by guide from clean.data.2 to make clean.data.3 
impute missing values at least two ways
start fitting models 

