---
title: "American Community Survery Project"
author: "Jonah Maroszek"
date: "3/24/2021"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(party)
library(knitr)
library(magrittr)
```
## Introduction
In this project I will use data for the state of Tennessee collected by ACS (American Community Survery) in order to estimate the mean of the variable INTP, which measures interest, dividends, and rental income. The initial data set contains 68,415 observations with 288 variables, though not all will be needed in predicting INTP. There are also a large number of missing values in this data set. I will be using multiple methods of imputation to correct for these deficiencies, as well as multiple methods to predict INTP.

```{r Data Import and Diagnosis, cache=TRUE, include=FALSE}
#will need to correct column types later on 
data = read_csv("psam_p47.csv", guess_max = 10000)
initial.dimensions = dim(data)

#finds variables where every value is the same to exclude from analysis
all.same = apply(data, 2, function(a) length(unique(a))==1)
cols.all.same = data[all.same]
```

## Data Cleaning
There are 8 variables in the data set that either have only one unique value, or they are a unique identifier, and thus not useful for prediction of INTP. These variables are: ADJINC, DIVISION, FANCP, REGION, RT, SERIALNO, SPORDER, ST.  

In addition to these variables, the variables PWGTP1-80 have been excluded, as these are sampling weights that are not needed for this analysis.  

Of the remaining variables, 43 of them have at least half of the values missing. The top ten variables that have the most missing values can be seen in the following table.
```{r Data Cleaning, echo=FALSE, cache=TRUE}
#First Phase of Data cleaning
vars.to.drop = c("ADJINC", "DIVISION", "FANCP", "REGION", "RT", "SERIALNO", "SPORDER", "ST")
clean.data.1 = data %>% 
  select(-all_of(vars.to.drop)) %>% 
  select(-num_range("PWGTP", 1:80)) 

dim.clean.data.1 = dim(clean.data.1)

#find proportion of missing values for remaining variables
missing.vals.logi = apply(clean.data.1,2, is.na)
prop.missing.vals = apply(missing.vals.logi, 2, sum) / dim.clean.data.1[1]
large.prop.missing.vals = prop.missing.vals[prop.missing.vals > 0.5]
top.ten.missing.vals = sort(large.prop.missing.vals, decreasing = TRUE)[1:10]
kable(top.ten.missing.vals, col.names = c("Proportion Missing"), caption = "Variables and Proportion of Missing Values", digits = 3)
```

```{r Variable Type Correction, include=FALSE, cache = TRUE}
num.vars = c("PWGTP","INTP", "AGEP", "JWMNP", "OIP","JWRIP","PAP", "OIP", "RETP","SEMP", "SSIP", "SSP", "WKWN", "WKHP", "WAGP","PINCP", "PERNP","POVPIP", "CITWP", "MARHYP", "YOEP") #numerical variables 
varnames.clean = names(clean.data.1) #get column names of clean data frame
cat.vars = varnames.clean[!(varnames.clean %in% num.vars)] #all categorical variables

#this data set has correct column types 
clean.data.2 = clean.data.1 %>% 
  mutate(across(all_of(num.vars), as.double)) %>% #changes type to double 
  mutate(across(all_of(cat.vars), as.character)) #changes type to character 

write_csv(clean.data.2, "cleanData.csv") #export clean data (200 variables) file to use in guide

```

```{r Subset creation for importance scoring, include = FALSE, cache = TRUE}
#take a small, random sample from the cleaned data frame in order to decrease run time of guide importance scoring algorithm
dim.clean.data.2 = dim(clean.data.2)
subset.indices = sample(1:dim.clean.data.2[1], dim.clean.data.2[1]/4) #randomly picked subset 
subset = clean.data.2[subset.indices,] #1/4 the size of original data frame
write_csv(subset,"QuarterSubset.csv") #export clean data file to use in guide
```

```{r create guide description file for cleaned data set, include = FALSE, cache = TRUE}
#create guide description file for clean.data.2
nvar = ncol(clean.data.2)
varnames = names(clean.data.2)
roles = rep("c", nvar) #default is categorical vars
roles[varnames %in% num.vars] = "n" #numerical variables
roles[varnames ==  "INTP"] = "d" #dependent variable
roles[varnames == "PWGTP"] = "w" #sampling weight variable
roles[varnames %in% vars.to.drop] = "x" #variables to exclude from analysis

#produce GUIDE Description File
write("cleanData.csv", file = "GUIDEdesc.txt")
write("NA",file="GUIDEdesc.txt",append=TRUE)
write("2",file="GUIDEdesc.txt",append=TRUE)
write.table(cbind(1:nvar,varnames,roles),file="GUIDEdesc.txt",
row.names=FALSE,col.names=FALSE,quote=FALSE,append=TRUE)
```
Of the remaining 200 variables, 123 variables were found to be unimportant in predicting INTP by GUIDE's importance scoring algorithm. The 20 most important variables can be seen in the following figure. After excluding the unimportant variables, the data set now contains 77 variables that will be used in the subsequent analysis.

```{r Plot Importance Scores, echo=FALSE, cache=FALSE}
par(las=1,mar=c(5,12,4,2),cex=0.75)
leg.col <- c("orange","yellow")
leg.txt <- c("highly important","likely important")
x <- read.table("impScores.txt",header=TRUE)
score <- x$Score
vars <- x$Variable
type <- x$Type
barcol <- rep("orange",length(vars))
barcol[type == "L"] <- "yellow"
barcol[type == "U"] <- "cyan"
n <- sum(x$Type != "U")
barplot(rev(score[1:20]),names.arg=rev(vars[1:20]),
col=rev(barcol[1:20]),horiz=TRUE,xlab="GUIDE importance scores", main = "Top 20 Important Variables")
abline(v=1,col="red",lty=2)
legend("bottomright",legend=leg.txt,fill=leg.col)

```


The top three variables for predicting INTP are PINCP, AGEP, and POVPIP. PINCP gives a persons total income, AGEP is a person's age, and POVPIP is an income to poverty ratio metric. It seems that all of these may be measuring directly or indirectly a person's ability to earn, and thus invest, money. The connection is very clear for PINCP, but is more subtle in AGEP. Typically people do not invest a substantial amount of money before the age of 20, and as their portfolio grows (in dollars and in time), they are more likely to earn money at a faster rate thanks to compound interest. POVPIP is also logically connected to a person's monetary standing, and therefore is unsurprisingly useful in predicting INTP.


```{r Final Data Cleanse, include = FALSE, cache = TRUE}
#extract names of significant variables found in GUIDE to get final version of the data set for analysis
signifVarsOneString = read_lines("cleanDataFinal.txt", skip = 4, n_max = 1)
signif.Vars = strsplit(signifVarsOneString, split = " ")[[1]]
final.data = clean.data.2[signif.Vars]
write_csv(final.data, "finalData.csv") #all unimportant variables removed; 77 remaining
```

```{r GUIDE classification tree, include= FALSE, cache = TRUE}
#create flag variable that tells if INTP has missing value on clean.data.2 ("cleanData.csv") to use for classification tree/forest
#note i am using clean.data.2 but the guide description file only includes the variables present in finalData and the flag variable for INTP
clean.data.2.with.flag = clean.data.2 %>% 
  mutate(is.missing.intp = is.na(INTP))

write_csv(clean.data.2.with.flag, "cleanData2WFlag.csv") #export data to use in GUIDE

#update description file to include is.missing.intp as dependent variable and remove intp from this part
```

```{r IPW Mean Estimation, include = FALSE, cache = TRUE}
#IPW method with classification GUIDE Tree
zclass = read.table("classfit.txt", header = TRUE)
p.nonmissing = zclass[,5] #prob INTP is non-missing
group = !is.na(clean.data.2$INTP) #nonmissing INTP obs
weights = clean.data.2$PWGTP
ipw = sum(weights[group]*clean.data.2$INTP[group]/p.nonmissing[group])/sum(weights[group]/p.nonmissing[group])

#IWP method with classification GUIDE Forest
zclass.forest = read.table("classfitForest.txt", header = TRUE)
p.forest = zclass.forest[,2]
ipw.forest = sum(weights[group]*clean.data.2$INTP[group]/p.forest[group])/sum(weights[group]/p.forest[group])
```

```{r}

```





