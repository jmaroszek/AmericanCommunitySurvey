---
title: "American Community Survery Project"
author: "Jonah Maroszek"
date: "3/24/2021"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(party)
library(knitr)
library(magrittr)
```
## Introduction
In this project I will use data for the state of Tennessee collected by ACS (American Community Survery) in order to estimate the mean of the variable INTP, which measures interest, dividends, and rental income. The initial data set contains 68,415 observations with 288 variables, though not all will be needed in predicting INTP. There are also a large number of missing values in this data set. I will be using multiple methods of imputation to correct for these deficiencies, as well as multiple methods to predict INTP.

```{r Data Import and Diagnosis, cache=TRUE, include=FALSE}
#will need to correct column types later on 
data = read_csv("psam_p47.csv", guess_max = 10000)
initial.dimensions = dim(data)

#finds variables where every value is the same to exclude from analysis
all.same = apply(data, 2, function(a) length(unique(a))==1)
cols.all.same = data[all.same]
```

## Data Cleaning
There are 8 variables in the data set that either have only one unique value, or they are a unique identifier, and thus not useful for prediction of INTP. These variables are: ADJINC, DIVISION, FANCP, REGION, RT, SERIALNO, SPORDER, ST.  

In addition to these variables, the variables PWGTP1-80 have been excluded, as these are sampling weights that are not needed for this analysis.  

Of the remaining variables, 43 of them have at least half of the values missing. The top ten variables that have the most missing values can be seen in the following table.
```{r Data Cleaning, echo=FALSE, cache=TRUE}
#First Phase of Data cleaning
vars.to.drop = c("ADJINC", "DIVISION", "FANCP", "REGION", "RT", "SERIALNO", "SPORDER", "ST")
clean.data.1 = data %>% 
  select(-all_of(vars.to.drop)) %>% 
  select(-num_range("PWGTP", 1:80)) 

dim.clean.data.1 = dim(clean.data.1)

#find proportion of missing values for remaining variables
missing.vals.logi = apply(clean.data.1,2, is.na)
prop.missing.vals = apply(missing.vals.logi, 2, sum) / dim.clean.data.1[1]
large.prop.missing.vals = prop.missing.vals[prop.missing.vals > 0.5]
top.ten.missing.vals = sort(large.prop.missing.vals, decreasing = TRUE)[1:10]
kable(top.ten.missing.vals, col.names = c("Proportion Missing"), caption = "Variables and Proportion of Missing Values", digits = 3)
```

```{r Variable Type Correction, include=FALSE, cache = TRUE}
num.vars = c("PWGTP","INTP", "AGEP", "JWMNP", "OIP","JWRIP","PAP", "OIP", "RETP","SEMP", "SSIP", "SSP", "WKWN", "WKHP", "WAGP","PINCP", "PERNP","POVPIP", "CITWP", "MARHYP", "YOEP") #numerical variables 
varnames.clean = names(clean.data.1) #get column names of clean data frame
cat.vars = varnames.clean[!(varnames.clean %in% num.vars)] #all categorical variables

#this data set has correct column types 
clean.data.2 = clean.data.1 %>% 
  mutate(across(all_of(num.vars), as.double)) %>% #changes type to double 
  mutate(across(all_of(cat.vars), as.character)) #changes type to character 

write_csv(clean.data.2, "cleanData.csv") #export clean data (200 variables) file to use in guide

```

```{r Subset creation for importance scoring, include = FALSE, cache = TRUE}
#take a small, random sample from the cleaned data frame in order to decrease run time of guide importance scoring algorithm
dim.clean.data.2 = dim(clean.data.2)
subset.indices = sample(1:dim.clean.data.2[1], dim.clean.data.2[1]/4) #randomly picked subset 
subset = clean.data.2[subset.indices,] #1/4 the size of original data frame
write_csv(subset,"QuarterSubset.csv") #export clean data file to use in guide
```

```{r create guide description file for cleaned data set, include = FALSE, cache = TRUE}
#create guide description file for clean.data.2
nvar = ncol(clean.data.2)
varnames = names(clean.data.2)
roles = rep("c", nvar) #default is categorical vars
roles[varnames %in% num.vars] = "n" #numerical variables
roles[varnames ==  "INTP"] = "d" #dependent variable
roles[varnames == "PWGTP"] = "w" #sampling weight variable
roles[varnames %in% vars.to.drop] = "x" #variables to exclude from analysis

#produce GUIDE Description File
write("cleanData.csv", file = "GUIDEdesc.txt")
write("NA",file="GUIDEdesc.txt",append=TRUE)
write("2",file="GUIDEdesc.txt",append=TRUE)
write.table(cbind(1:nvar,varnames,roles),file="GUIDEdesc.txt",
row.names=FALSE,col.names=FALSE,quote=FALSE,append=TRUE)
```
Of the remaining 200 variables, 123 variables were found to be unimportant in predicting INTP by GUIDE's importance scoring algorithm. The 20 most important variables can be seen in the following figure.
```{r Plot Importance Scores, include = FALSE, cache = TRUE}
par(las=1,mar=c(5,12,4,2),cex=0.75)
leg.col <- c("orange","yellow")
leg.txt <- c("highly important","likely important")
x <- read.table("impScores.txt",header=TRUE)
score <- x$Score
vars <- x$Variable
type <- x$Type
barcol <- rep("orange",length(vars))
barcol[type == "L"] <- "yellow"
barcol[type == "U"] <- "cyan"
n <- sum(x$Type != "U")
barplot(rev(score[1:20]),names.arg=rev(vars[1:20]),
col=rev(barcol[1:20]),horiz=TRUE,xlab="GUIDE importance scores", main = "Top 20 Important Variables")
abline(v=1,col="red",lty=2)
legend("bottomright",legend=leg.txt,fill=leg.col)

```

After excluding the unimportant variables, the data set now contains 77 variables.


```{r Final Data Cleanse}
#extract names of significant variables found in GUIDE to get final version of the data set for analysis
signifVarsOneString = read_lines("cleanDataFinal.txt", skip = 4, n_max = 1)
signif.Vars = strsplit(signifVarsOneString, split = " ")[[1]]
final.data = clean.data.2[signif.Vars]


```


# Short term to do list
clean data further by removing unimportant variables found in guide importance scoring (and update guide description file)
fit guide tree and forest



