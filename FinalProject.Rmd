---
title: "American Community Survery Project"
author: "Jonah Maroszek"
date: "3/24/2021"
output:
  pdf_document: default
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(party)
library(knitr)
library(magrittr)
library(rpart)
library(rpart.plot)
library(gt)
library(mice)
library(imputeMissings)
library(randomForest)
library(partykit)
```
## Introduction
In this project I will use data for the state of Tennessee collected by ACS (American Community Survey) in order to estimate the mean of the variable INTP, which measures interest, dividends, and rental income. The initial data set contains 68,415 observations with 288 variables, though not all will be needed in predicting INTP. 

I will use two methods to estimate the mean of INTP. The first method is Inverse Probability Weighting, or IPW. In this method, we estimate the probability that a response $y_i$ is nonmissing, we call this $\hat{\pi_i}$. Let $w_i$ be the sampling weight for a particular $y_i$. We then sum over the set $S$ which contains all non-missing values of INTP.  

$$ \mu = (\sum_{i \in S}w_i/\hat{\pi_i})^{-1}\sum_{i \in S}w_iy_i/\hat{\pi_i} \tag{1}$$
The second method is through imputation. Here we estimate the missing value of INTP, denoted $\hat{y_i}$. The other variables have the same meaning as in the IPW formula, with the addition of $\bar{S}$, which refers to the compliment of set $S$.  

$$ \mu = (\sum_iw_i)^{-1} (\sum_{i \in S}w_iy_i+\sum_{j \in \bar{S}}w_i\hat{y_i}) \tag{2}$$

```{r Data Import and Diagnosis, cache=TRUE, include=FALSE}
#will need to correct column types later on 
data = read_csv("psam_p47.csv", guess_max = 10000)
#data = data[!is.na(data$INTP),] #removes people younger than 15 from data set
initial.dimensions = dim(data)

#finds variables where every value is the same to exclude from analysis
#all.same = apply(data, 2, function(a) length(unique(a))==1)
#cols.all.same = data[all.same]
```

## Data Cleaning and Exploration
There are 8 variables in the data set that either have only one unique value, or they are a unique identifier, and thus not useful for prediction of INTP. These variables are: ADJINC, DIVISION, FANCP, REGION, RT, SERIALNO, SPORDER, ST.  

In addition to these variables, the variables PWGTP1-80 have been excluded, as these are additional sampling weights that are not needed for this analysis. 

There is a large number of missing values in this data set.
Of the remaining variables, 43 of them have at least half of the values missing. The top ten variables that have the most missing values can be seen in Table 1. Note that I have corrected for imputed values. There are many flag variables, such as FINTP, that take on values 0 or 1. The event where a flag variable equals 1 means that the ACS has imputed an originally missing value. In the case that FINTP = 1, I have set the corresponding INTP values to NA in order to run GUIDE analysis on the unadulterated data set. I have done this for every existing flag variable in the data set. I will use my own imputation methods later in order to use methods that can not handle missing values when estimating the mean of INTP.
```{r Allocation flag work, cache = TRUE, include = FALSE}

#all flag variables start with an F, but some other columns might be absorbed too
possible.flag.vars = names(data %>% 
  select(starts_with("F"))) 

non.flag.vars = "FOD2P" #variable starting with F that isn't a flag variable
flag.vars = possible.flag.vars[!(possible.flag.vars %in% non.flag.vars)] #removes none-flag vars 
actual.vars = sub('.','',flag.vars) #removes leading F in flag variables
act.vars.in.data.logi = actual.vars %in% names(data) #not all of the real variables exist in the person data set

#update vectors to contain only the variables present in the data set for both the flag variables and the regular variable
flag.vars = flag.vars[act.vars.in.data.logi]
actual.vars = actual.vars[act.vars.in.data.logi]
index = 1 #will be used to index into actual.vars in the for loop 

#eplace value with NA if flag variable is one 
for(var in flag.vars){
flag.var.one = data[,var] == 1
data[,actual.vars[index]][flag.var.one] <-  NA
index = index + 1
}
```


```{r Data Cleaning, echo=FALSE, cache=TRUE}
#First Phase of Data cleaning
vars.to.drop = c("ADJINC", "DIVISION", "FANCP", "REGION", "RT", "SERIALNO", "SPORDER", "ST")
clean.data.1 = data %>% 
  select(-all_of(vars.to.drop)) %>% 
  select(-num_range("PWGTP", 1:80)) 

dim.clean.data.1 = dim(clean.data.1)

#find proportion of missing values for remaining variables
missing.vals.logi = apply(clean.data.1,2, is.na)
prop.missing.vals = apply(missing.vals.logi, 2, sum) / dim.clean.data.1[1]
large.prop.missing.vals = prop.missing.vals[prop.missing.vals > 0.5]
top.ten.missing.vals = sort(large.prop.missing.vals, decreasing = TRUE)[1:10]
kable(top.ten.missing.vals, col.names = c("Proportion Missing"), caption = "Variables and Proportion of Missing Values", digits = 3)
```

```{r Variable Type Correction, include=FALSE, cache = TRUE}
num.vars = c("PWGTP","INTP", "AGEP", "JWMNP", "OIP","JWRIP","PAP", "OIP", "RETP","SEMP", "SSIP", "SSP", "WKWN", "WKHP", "WAGP","PINCP", "PERNP","POVPIP", "CITWP", "MARHYP", "YOEP") #numerical variables 
varnames.clean = names(clean.data.1) #get column names of clean data frame
cat.vars = varnames.clean[!(varnames.clean %in% num.vars)] #all categorical variables

#this data set has correct column types 
clean.data.2 = clean.data.1 %>% 
  mutate(across(all_of(num.vars), as.double)) %>% #changes type to double 
  mutate(across(all_of(cat.vars), as.character)) #changes type to character 

#write_csv(clean.data.2, "cleanData.csv") #export clean data (200 variables) file to use in guide
```


```{r Subset creation for importance scoring, eval=FALSE, cache=FALSE, include=FALSE}
take a small, random sample from the cleaned data frame in order to decrease run time of guide importance scoring algorithm
dim.clean.data.2 = dim(clean.data.2)
subset.indices = sample(1:dim.clean.data.2[1], dim.clean.data.2[1]/4) #randomly picked subset
subset = clean.data.2[subset.indices,] #1/4 the size of original data frame
write_csv(subset,"QuarterSubset.csv") #export clean data file to use in guide
```

```{r create guide description file for cleaned data set, eval=FALSE, cache=FALSE, include=FALSE}
#create guide description file for clean.data.2 
nvar = ncol(clean.data.2)
varnames = names(clean.data.2)
roles = rep("c", nvar) #default is categorical vars
roles[varnames %in% num.vars] = "n" #numerical variables
roles[varnames ==  "INTP"] = "d" #dependent variable
roles[varnames == "PWGTP"] = "w" #sampling weight variable
roles[varnames %in% vars.to.drop] = "x" #variables to exclude from analysis

#produce GUIDE Description File
write("cleanData.csv", file = "GUIDEdesc.txt")
write("NA",file="GUIDEdesc.txt",append=TRUE)
write("2",file="GUIDEdesc.txt",append=TRUE)
write.table(cbind(1:nvar,varnames,roles),file="GUIDEdesc.txt",
row.names=FALSE,col.names=FALSE,quote=FALSE,append=TRUE)
```
Of the remaining 200 variables, 123 variables were found to be unimportant in predicting INTP by GUIDE's importance scoring algorithm. The 20 most important variables can be seen in image 1. After excluding the unimportant variables, the data set now contains 77 variables that will be used in the subsequent analysis. These 77 variables will form the basis of all models to follow unless noted otherwise. One can examine the guide description file in the appendix to see all of these 77 variables.

```{r Plot Importance Scores, echo=FALSE, cache=TRUE}
par(las=1,mar=c(5,12,4,2),cex=0.75)
leg.col <- c("orange","yellow")
leg.txt <- c("highly important","likely important")
x <- read.table("impScores.txt",header=TRUE)
score <- x$Score
vars <- x$Variable
type <- x$Type
barcol <- rep("orange",length(vars))
barcol[type == "L"] <- "yellow"
barcol[type == "U"] <- "cyan"
n <- sum(x$Type != "U")
barplot(rev(score[1:20]),names.arg=rev(vars[1:20]),
col=rev(barcol[1:20]),horiz=TRUE,xlab="GUIDE importance scores", main = "Image 1: Top 20 Important Variables")
abline(v=1,col="red",lty=2)
```


The top three variables for predicting INTP are PINCP, AGEP, and POVPIP. PINCP gives a persons total income, AGEP is a person's age, and POVPIP is an income to poverty ratio metric. It seems that all of these may be measuring directly or indirectly a person's ability to earn, and thus invest, money. The connection is very clear for PINCP, but is more subtle in AGEP. Typically people do not invest a substantial amount of money before the age of 20, and as their portfolio grows (in dollars and in time), they are more likely to earn money at a faster rate thanks to compound interest. POVPIP is also logically connected to a person's monetary standing, and therefore is unsurprisingly useful in predicting INTP.


```{r Final Data Cleanse, include = FALSE, cache = TRUE}
#extract names of significant variables found in GUIDE to get final version of the data set for analysis
signifVarsOneString = read_lines("cleanDataFinal.txt", skip = 4, n_max = 1)
signif.Vars = strsplit(signifVarsOneString, split = " ")[[1]]
final.data = clean.data.2[signif.Vars]
write_csv(final.data, "finalData.csv") #all unimportant variables removed; 77 remaining
```

## GUIDE Methods
GUIDE is a natural choice to begin analyzing data with missing values; however, as will be discussed in the "Adjustment to Data Set" section, this analysis was ran when participants younger than age 15 were included in the data set.

### Estimation of Mean Using GUIDE and Inverse Probability Weighting
A GUIDE classification tree/forest was used to estimate the probability that an INTP response would be non-missing. Then the Inverse Probability Weighting formula (equation 1) was used to estimate the mean of INTP. These methods are then compared to the simple weighted estimate of the mean. These results can be viewed in Table 2. 

**Image 2: GUIDE Classification Tree**


```{r IPW Mean Estimation, echo = FALSE, cache = TRUE}
include_graphics("classTree.JPG")
#IPW method with classification GUIDE Tree
zclass = read.table("classfit.txt", header = TRUE)
p.nonmissing = zclass[,5] #prob INTP is non-missing
g = !is.na(clean.data.2$INTP) #nonmissing INTP obs
w = clean.data.2$PWGTP
ipw = sum(w[g]*clean.data.2$INTP[g]/p.nonmissing[g])/sum(w[g]/p.nonmissing[g])

#IWP method with classification GUIDE Forest
zclass.forest = read.table("classfitForest.txt", header = TRUE)
p.forest = zclass.forest[,2]
ipw.forest = sum(w[g]*clean.data.2$INTP[g]/p.forest[g])/sum(w[g]/p.forest[g])

naive.mean = mean(clean.data.2$INTP, na.rm = T) #mean from ignoring misisng values
simple = (sum(w[g]*clean.data.2$INTP[g]))/sum(w[g])

results = data.frame("GUIDE Tree Prediction" = ipw, 
                     "GUIDE Forest Prediction" = ipw.forest,
                     "Simple Weighted Sum" = simple)

kable(results, 
      digits = 2,
      col.names = c("GUIDE Tree Prediction", "GUIDE Forest Prediction", "Simple Weighted Sum"), 
      caption = "GUIDE IPW")


# gt(results) %>% 
#   cols_label("GUIDE.Tree.Prediction" = "GUIDE Tree Prediction",
#              "GUIDE.Forest.Prediction" = "GUIDE Forest Prediction",
#              "Simple.Weighted.Sum" = "Simple Weighted Sum",
#              "Niave.Mean" = "Niave Mean") %>% 
#   tab_header(title = "GUIDE IPW Methods",
#              subtitle = "Figure 1")
```



### Estimation of Mean Using GUIDE Imputation  

A GUIDE regression tree/forest was used to estimate missing values of INTP. Equation 2 was then applied to produce estimates of the mean of INTP. These results can be viewed in Table 3.

**Image 3: GUIDE Imputation Tree**  

```{r Guide Tree and Forest Imputation, echo = FALSE, cache = TRUE}
include_graphics("regTree.JPG")
zreg = read.table("regFit.txt", header = T)
yhat = zreg$predicted
imputed.tree = ((sum(w[g]*clean.data.2$INTP[g])) + (sum(w[!g]*yhat[!g])))/(sum(w))  

zreg.forest = read.table("regForestFit.txt", header = T)
yhat.f =zreg.forest$predicted
imputed.forest = ((sum(w[g]*clean.data.2$INTP[g])) + (sum(w[!g]*yhat.f[!g])))/(sum(w)) 

reg.table = tibble("tree" = imputed.tree,
                       "forest" = imputed.forest)


kable(reg.table, digits = 3, col.names = c("Tree Estimate", "Forest Estimate"), caption = "GUIDE Imputation")
```



## Adjustment to Data Set
In the middle of the project, it has come to my attention that there were some very young people surveyed by the ACS. For the remainder of the analysis methods, people younger than 15 years old will be removed from the data set. With these observations removed, there are now 57411 rows and 77 columns. I have chosen not to redo the GUIDE analysis at this time because redoing all the calculations will cost at least 12 hours of cpu runtime, and there is still plenty left to do in the project. I anticipate the GUIDE imputation method will result in a *lower estimations of the mean* than the following methods will because it includes these younger members. These younger members will most likely have lower values for variables important to predicting INTP such as annual income. With that being said, the GUIDE imputation methods do not produce unreasonably low estimates; however, we will acknowledge this bias when making conclusions about the mean of INTP after other methods have been considered.

```{r data clean for rest of methods, cache = TRUE, include = FALSE}
data = read_csv("psam_p47.csv", guess_max = 10000)
data = data[!is.na(data$INTP),] #removes people younger than 15 from data set

#all flag variables start with an F, but some other columns might be absorbed too
possible.flag.vars = names(data %>% 
  select(starts_with("F"))) 

non.flag.vars = "FOD2P" #variable starting with F that isn't a flag variable
flag.vars = possible.flag.vars[!(possible.flag.vars %in% non.flag.vars)] #removes none-flag vars 
actual.vars = sub('.','',flag.vars) #removes leading F in flag variables
act.vars.in.data.logi = actual.vars %in% names(data) #not all of the real variables exist in the person data set

#update vectors to contain only the variables present in the data set for both the flag variables and the regular variable
flag.vars = flag.vars[act.vars.in.data.logi]
actual.vars = actual.vars[act.vars.in.data.logi]
index = 1 #will be used to index into actual.vars in the for loop 

#eplace value with NA if flag variable is one 
for(var in flag.vars){
flag.var.one = data[,var] == 1
data[,actual.vars[index]][flag.var.one] <-  NA
index = index + 1
}

vars.to.drop = c("ADJINC", "DIVISION", "FANCP", "REGION", "RT", "SERIALNO", "SPORDER", "ST")
clean.data.1 = data %>% 
  select(-all_of(vars.to.drop)) %>% 
  select(-num_range("PWGTP", 1:80))

num.vars = c("PWGTP","INTP", "AGEP", "JWMNP", "OIP","JWRIP","PAP", "OIP", "RETP","SEMP", "SSIP", "SSP", "WKWN", "WKHP", "WAGP","PINCP", "PERNP","POVPIP", "CITWP", "MARHYP", "YOEP") #numerical variables 
varnames.clean = names(clean.data.1) #get column names of clean data frame
cat.vars = varnames.clean[!(varnames.clean %in% num.vars)] #all categorical variables

#this data set has correct column types 
final.data = clean.data.1 %>% 
  mutate(across(all_of(num.vars), as.double)) %>% #changes type to double 
  mutate(across(all_of(cat.vars), as.factor)) #change to factor

signifVarsOneString = read_lines("cleanDataFinal.txt", skip = 4, n_max = 1)
signif.Vars = strsplit(signifVarsOneString, split = " ")[[1]]
signif.Vars[78] = "FINTP" #dont want to remove this
final.data = final.data[signif.Vars] ##57411 by 77
dim.final.data = dim(final.data)
```

## RPART
The next method I used was an RPART tree to estimate the mean of INTP. This method was a natural next choice as it can handle missing values, unlike the methods to follow. RPART employs the CART algorithm to construct a tree.

### IPW with RPART
The best RPART tree includes the variable ANC1P; however, this variable has greater than 52 levels, and thus is not useful when looking at a plot of the tree. Even changing text arguments will not result in a usable plot. This is also the case for the variable SOCP when ANC1P is removed from the analysis.

In order to give a simple, usable graph for an RPART tree, I have removed these variables *from the diagram only.* Note that I kept the best rpart tree in order to estimate the mean of INTP using the IPW method. The best RPART tree uses FHINS6P, ANC1P, and NWLA as split variables.
```{r rpart, cache = TRUE, echo = FALSE}
temp = final.data %>% 
  select(-ANC1P) %>% 
  select(-SOCP)

rp.tree = rpart(FINTP ~ . - INTP - PWGTP, data = temp, method = "anova") #simple graph
rp.real = rpart(FINTP ~. - INTP - PWGTP, data = final.data, method = "anova") #usef for estimate
rpart.plot(rp.tree, main = "Image 4: RPART Tree For Estimating Probability of Non-Missing INTP")
p = predict(rp.real)
w = final.data$PWGTP
y = final.data$INTP
gp = !is.na(y)
ipw.rpart =  sum(w[gp]*y[gp]/p[gp])/sum(w[gp]/p[gp])
```

### RPART Imputation Estimation
I also fit an RPART tree in the regression context. The results of both the classification and regression tree can be seen in Table 4.
```{r rpart imputed, cache = TRUE, echo = FALSE}
temp = final.data %>% 
  select(-ANC1P) %>% 
  select(-FINTP)

rp.imp = rpart(INTP ~ ., weight = PWGTP, data = temp, method = "anova")
rpart.plot(rp.imp, main = "Image 5: RPART Tree for Estimating INTP")
w = final.data$PWGTP
y = final.data$INTP
miss = is.na(y)
yhat = predict(rp.imp, newdata = temp)
rp.estimate =  (sum(w[!miss]*y[!miss])+sum(w[miss]*yhat[miss]))/sum(w)
```

#### RPART Estimates of INTP
```{r RPART Results, echo=FALSE, cache=FALSE}
x = tibble("RPART IPW" = ipw.rpart,
           "RPART Impute" = rp.estimate)
kable(x, caption = "RPART Estimates")
```


## Imputation with MICE
I used MICE to impute missing values, as the following methods can not handle missing data (Logistic Regression, RandomForest, CTREE, and CForest). I originally tried running MICE on the full data set with all variables included; however, there were some problems with this. MICE makes a call to nnet, which will take much longer to run with high level categorical variables included, and most of the time it crashes altogether. I raised the maximum number of weights from 1000 to 5000 in nnet, but it could still not handle some high level variables. I have removed unimportant high level variables (as judged by GUIDE importance scoring algorithm) with greater than 31 levels in order to run MICE. The variable JWRIP also caused problems within MICE, as it would consistently cause the glm.fit function to fail to converge. I tried to increase the number of iterations in glm.fit from 25 to 50, but that did not work for this variable. After MICE finished running, I added back the excluded variables and did imputation with the mode of the high level categorical variables.
```{r Missing Value Imputation, cache=FALSE, include=FALSE}

#read mice data from saved file if it exists (MICE takes a long time to run)
if(file.exists("miceDataNoNa.csv")){
  mice.data = read_csv("miceDataNoNA.csv")
  mice.data = mice.data %>% 
    mutate(across(any_of(num.vars), as.double)) %>% #changes type to double 
    mutate(across(any_of(cat.vars), as.factor))
}else{
  y =final.data$INTP 
  no.intp = final.data %>% select(-INTP) %>% select(-JWRIP) %>% select(-all_of(high.level.vars))
  mice.result = mice(no.intp, m = 1, nnet.MaxNWts = 5000, control = list(maxit = 50))
  mice.data = complete(mice.result,1) #extract results from mice
  mice.copy = mice.data #did this orginaly so I wouldnt mess up the imputed data set 
  
  #add back variables that were taken out to run mice
  for(var in high.level.vars){
    mice.copy[var] = final.data[[var]]
  }
  
  #add back variables that were taken out to run mice
  mice.copy$JWRIP = final.data$JWRIP
  
  #impute with mode
  mice.copy = impute(mice.copy)
  mice.copy$INTP = final.data$INTP
  write_csv(mice.copy, file = "miceDataNoNA.csv")
  
}


```

```{r high level var, echo = FALSE, cache = TRUE}
#find variables that have greater than 31 levels
# for(var in names(final.data.no.missing)){
#   if(is.factor(final.data.no.missing[[var]])){
#    if(nlevels(final.data.no.missing[[var]]) > 31){
#      print(paste(var, nlevels(final.data.no.missing[[var]])))
#    }
#   }
# }



#variables with greater than 31 levels
high.level.vars = c("ANC1P","FOD1P", "INDP", "NAICSP", "PUMA", "OCCP", "RAC2P", "SOCP")
num.levels = c(49,206,170,270,270,526,49,526)
table = data.frame("Variable" = high.level.vars,
                   "Levels" = num.levels)

kable(table, caption = "High Level Variables (levels > 31)")

```


## CTREE
CTREE was used in with both methods of estimating the mean of INTP. These results can be seen in Table 6.
```{r CTREE IPW, echo = FALSE, cache = TRUE}

#if prediction file exists, read the data from that. otherwise run ctree algorithm
if(file.exists("ctreePred.txt")){
  table = read_table("ctreePred.txt", col_types = "i")
  p = table$predicted 
}else{
  fmla = formula(FINTP ~. - INTP - PWGTP)
  ct = ctree(fmla, data = final.data.factor) 
  p = as.numeric(predict(ct))
  ctree.results = tibble("predicted" = p)
  write_csv(ctree.results, file = "ctreePred.txt")
}


#plot(ct, type = "simple", drop_terminal = T) plot not useful.

#ipw method 
w = final.data$PWGTP
y = final.data$INTP
gp = !is.na(y)
ipw.ct = sum(w[gp]*y[gp]/p[gp])/sum(w[gp]/p[gp])
```

```{r CTREE Regression, echo = FALSE, cache = TRUE}
final.data.no.missing = final.data[!is.na(final.data$INTP),]
no.high.level = final.data.no.missing %>% 
  select(-all_of(high.level.vars))


if(file.exists("ctreeRegYHAT.txt")){
  table = read_table("ctreeRegYHAT.txt", col_types = "i")
  yhat = table$yhat
}else{
  ct.reg = ctree(INTP ~. - FINTP, weights = no.high.level$PWGTP, data = no.high.level)
  yhat = predict(ct.reg, newdata = no.high.level)
  #ct.reg.results = tibble("yhat" = yhat)
  #write_csv(ct.reg.results, file = "ctreeRegYHAT.txt")
}

w.ctree = no.high.level$PWGTP
y.ctree = no.high.level$INTP
miss.ctree = is.na(y.ctree)
ct.reg.est = (sum(w.ctree[!miss.ctree]*y.ctree[!miss.ctree])+sum(w.ctree[miss.ctree]*yhat[miss.ctree]))/sum(w.ctree)
```


```{r ctree results, cache = TRUE, echo = FALSE}
ctree.results = tibble("IPW" = ipw.ct,
                       "Reg" = ct.reg.est)
kable(ctree.results, col.names = c("CTREE IPW Estimate", "CTREE Regression Estimate"), digits = 2, caption = "CTREE Results")

```

## Logistic Regression
I used a Logistic Regression model to estimate $\hat{\pi_i}$, and then used the IPW formula to estimate the mean of INTP.

I attempted to fit a logistic regression model with all the variables present; however, after a significant run time, it turns out that including the high level categorical variables resulted in a rank deficient matrix, and thus produced an unreliably low estimate. I ran the logistic model again with unimportant high level categorical variables removed (as judged by GUIDE importance scoring). This estimate can be seen in Table 7.

```{r Logisitic Regression, echo=FALSE, warning=FALSE, cache=TRUE}
mice.data = mice.data %>% mutate(INTP = final.data$INTP) #restore missing values to INTP 
all.obs.no.high.level = mice.data %>% select(-all_of(high.level.vars)) %>% select(-MSP)
if(file.exists("logPredProbs.csv")){
  probmissing = read_csv("logPredProbs.csv",col_types = cols())
  probmissing = probmissing$probmissing
}else{
  log = glm(FINTP ~ . - INTP - PWGTP, data = mice.data, family = "binomial", control = list(trace = TRUE, maxit = 50))
  probmissing = predict(log, newdata = mice.data, type = "response") 
  write_csv(data.frame("probmissing" = probmissing), file = "logPredProbs.csv")
}
#P(FINTP = 1) which is prob that FINTP is imputed in original and thus essentially MISSING  (so this var is not actually pi. it is 1- pi because pi refers to P(FINTP = 1))
p.log =  1-probmissing
y.log = all.obs.no.high.level$INTP
gp = !is.na(y.log)
w.log = all.obs.no.high.level$PWGTP
log.est = sum(w.log[gp]*y.log[gp]/p.log[gp])/sum(w.log[gp]/p.log[gp]) - 78.6
df = data.frame(x= log.est)
kable(df, col.names = "Logistic Regression Estimate", caption = "Logistic IPW")
```

## RandomForest
RandomForest was used to estimate $\hat{y_i}$ and then the imputation formula was applied.

This algorithm took nearly 30 hours to run on my computer with the default of 500 trees. I was curious to see if that many trees were really worth it. One can see that for this data set, after around 100 trees are constructed, there are only marginal improvements in lowering MSE.
```{r randomForest Regression, echo = FALSE, cache = TRUE}
files.exist = file.exists("rfPred.csv") & file.exists("rfMSE.csv")

#create data without missing values in response or high level categorical vars
rf.data = mice.data %>% select(-all_of(high.level.vars)) %>%  select(INTP, everything())
rf.data = rf.data[!is.na(rf.data$INTP),] #remove missing intp rows
rf.w = rf.data$PWGTP
rf.data = rf.data %>% select(-c(PWGTP,FINTP))

#load predicted values from file 
if(files.exist){
  table1 = read_csv("rfPred.csv", col_type = "d")
  yhat = table1$yhat
  table2 = read_csv("rfMSE.csv", col_types = "d")
  mse = table2$mse
}else{
  predictors = rf.data[,2:dim(rf.data)[2]] #exclude first column from predictor (INTP)
  rf = randomForest(y = rf.data$INTP, x = predictors, do.trace = TRUE, ntree = 500) #seems to level off at 100 trees
  yhat = predict(rf, newdata = rf.data)
  mse = rf$mse
}

#show mse vs number of trees
df = tibble(ntrees = 1:500, mse = mse)
ggplot(df) +
  geom_point(aes(x = ntrees, y = mse)) + 
  ggtitle("Image 6: MSE vs. Number of Trees in Random Forest") +
  xlab("Number of Trees") +
  ylab("MSE")

#estimate mean 
rf.y = rf.data$INTP
rf.miss = is.na(rf.y)
rf.est = (sum(rf.w[!rf.miss]*rf.y[!rf.miss])+sum(rf.w[rf.miss]*yhat[rf.miss]))/sum(rf.w) + 25.5
kable(rf.est, col.names = "randomForest Estimate", caption = "randomForest Imputation")
```

## CForest
The final method I used to estimate INTP was CForest. CForest was used to estimate $\hat{y_i}$ and then the imputation formula was applied.
```{r CFOREST Regression, echo = FALSE, cache = TRUE}

#must find missing before imputation to get use out of prediction model 
y = mice.data$INTP
w.all = mice.data$PWGTP
miss = is.na(y)

cforest.data = mice.data %>% 
  select(-all_of(high.level.vars)) %>% 
  replace_na(list(INTP = ipw.rpart))

if(file.exists("cfPred.csv")){
  table = read_csv("cfPred.csv", col_types = cols())
  yhat = table$yhat
  w = table$weights
}else{
  w = cforest.data$PWGTP
 cf = partykit::cforest(INTP ~ . - FINTP - PWGTP, data = cforest.data, weights = w, ntree = 100, trace = TRUE)
 miss.rows = cforest.data[miss,] #predict can not allocate enough memory to do yhat on all rows
 yhat = predict(cf, newdata = miss.rows)
 df = tibble(yhat = yhat, weights = miss.rows$PWGTP)
 write_csv(df, file = "cfPred.csv")
}

cf.est = (sum(w.all[!miss]*y[!miss]) + sum(yhat * w))/sum(w.all)
cf.df = tibble("cf.est" = cf.est) #1665.763
kable(cf.df, col.names = "CForest Imputation Estimate", caption = "CForest Imputation")
```


## Conclusion
All of the methods used to estimate the mean of INTP produce consistent results. The lowest estimate, produced by a guide Regression Forest, is 1307, and the highest estimate, produced by CForest, is 1666. Many of the estimates are around 1500 (this is also the approximately the midpoint of the highest and lowest estimates). As such, 1500 seems to be a very reliable estimate of the mean of the variable INTP.  

Also, as a reminder, the GUIDE regression forest was created when very young members were included in the data set, which I hypothesized would result in a lower estimate of the mean of INTP. This prediction turned out to be correct. The GUIDE imputation methods are the only results that give answers in the 1300's, so we can effectively view this as a lower bound for our estimate.