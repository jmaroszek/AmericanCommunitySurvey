---
title: "American Community Survery Project"
author: "Jonah Maroszek"
date: "3/24/2021"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(party)
library(knitr)
library(magrittr)
library(rpart)
library(rpart.plot)
library(gt)
library(mice)
library(imputeMissings)
```
## Introduction
In this project I will use data for the state of Tennessee collected by ACS (American Community Survery) in order to estimate the mean of the variable INTP, which measures interest, dividends, and rental income. The initial data set contains 68,415 observations with 288 variables, though not all will be needed in predicting INTP. There are also a large number of missing values in this data set. I will be using multiple methods of imputation to correct for these deficiencies, as well as multiple methods to predict INTP.

```{r Data Import and Diagnosis, cache=TRUE, include=FALSE}
#will need to correct column types later on 
data = read_csv("psam_p47.csv", guess_max = 10000)
#data = data[!is.na(data$INTP),] #removes people younger than 15 from data set
initial.dimensions = dim(data)

#finds variables where every value is the same to exclude from analysis
#all.same = apply(data, 2, function(a) length(unique(a))==1)
#cols.all.same = data[all.same]
```

## Data Cleaning
There are 8 variables in the data set that either have only one unique value, or they are a unique identifier, and thus not useful for prediction of INTP. These variables are: ADJINC, DIVISION, FANCP, REGION, RT, SERIALNO, SPORDER, ST.  

In addition to these variables, the variables PWGTP1-80 have been excluded, as these are sampling weights that are not needed for this analysis.  

Of the remaining variables, 43 of them have at least half of the values missing. The top ten variables that have the most missing values can be seen in the following table. Note that I have corrected for imputed values. There are many flag variables, such as FINTP, that take on values 0 or 1. In the case that FINTP = 1, I have set the corresponding INTP values to NA in order to run GUIDE analysis on the unadulterated data set. I have done this for every existing flag variable in the data set. I will use my own imputation methods later in order to use methods that can not handle missing values when estimating the mean of INTP.
```{r Allocation flag work, cache = TRUE, include = FALSE}

#all flag variables start with an F, but some other columns might be absorbed too
possible.flag.vars = names(data %>% 
  select(starts_with("F"))) 

non.flag.vars = "FOD2P" #variable starting with F that isn't a flag variable
flag.vars = possible.flag.vars[!(possible.flag.vars %in% non.flag.vars)] #removes none-flag vars 
actual.vars = sub('.','',flag.vars) #removes leading F in flag variables
act.vars.in.data.logi = actual.vars %in% names(data) #not all of the real variables exist in the person data set

#update vectors to contain only the variables present in the data set for both the flag variables and the regular variable
flag.vars = flag.vars[act.vars.in.data.logi]
actual.vars = actual.vars[act.vars.in.data.logi]
index = 1 #will be used to index into actual.vars in the for loop 

#eplace value with NA if flag variable is one 
for(var in flag.vars){
flag.var.one = data[,var] == 1
data[,actual.vars[index]][flag.var.one] <-  NA
index = index + 1
}
```


```{r Data Cleaning, echo=FALSE, cache=TRUE}
#First Phase of Data cleaning
vars.to.drop = c("ADJINC", "DIVISION", "FANCP", "REGION", "RT", "SERIALNO", "SPORDER", "ST")
clean.data.1 = data %>% 
  select(-all_of(vars.to.drop)) %>% 
  select(-num_range("PWGTP", 1:80)) 

dim.clean.data.1 = dim(clean.data.1)

#find proportion of missing values for remaining variables
missing.vals.logi = apply(clean.data.1,2, is.na)
prop.missing.vals = apply(missing.vals.logi, 2, sum) / dim.clean.data.1[1]
large.prop.missing.vals = prop.missing.vals[prop.missing.vals > 0.5]
top.ten.missing.vals = sort(large.prop.missing.vals, decreasing = TRUE)[1:10]
kable(top.ten.missing.vals, col.names = c("Proportion Missing"), caption = "Variables and Proportion of Missing Values", digits = 3, label = "Figure 1")
```

```{r Variable Type Correction, include=FALSE, cache = TRUE}
num.vars = c("PWGTP","INTP", "AGEP", "JWMNP", "OIP","JWRIP","PAP", "OIP", "RETP","SEMP", "SSIP", "SSP", "WKWN", "WKHP", "WAGP","PINCP", "PERNP","POVPIP", "CITWP", "MARHYP", "YOEP") #numerical variables 
varnames.clean = names(clean.data.1) #get column names of clean data frame
cat.vars = varnames.clean[!(varnames.clean %in% num.vars)] #all categorical variables

#this data set has correct column types 
clean.data.2 = clean.data.1 %>% 
  mutate(across(all_of(num.vars), as.double)) %>% #changes type to double 
  mutate(across(all_of(cat.vars), as.character)) #changes type to character 

#write_csv(clean.data.2, "cleanData.csv") #export clean data (200 variables) file to use in guide
```


```{r Subset creation for importance scoring, eval=FALSE, cache=FALSE, include=FALSE}
take a small, random sample from the cleaned data frame in order to decrease run time of guide importance scoring algorithm
dim.clean.data.2 = dim(clean.data.2)
subset.indices = sample(1:dim.clean.data.2[1], dim.clean.data.2[1]/4) #randomly picked subset
subset = clean.data.2[subset.indices,] #1/4 the size of original data frame
write_csv(subset,"QuarterSubset.csv") #export clean data file to use in guide
```

```{r create guide description file for cleaned data set, eval=FALSE, cache=FALSE, include=FALSE}
#create guide description file for clean.data.2 
nvar = ncol(clean.data.2)
varnames = names(clean.data.2)
roles = rep("c", nvar) #default is categorical vars
roles[varnames %in% num.vars] = "n" #numerical variables
roles[varnames ==  "INTP"] = "d" #dependent variable
roles[varnames == "PWGTP"] = "w" #sampling weight variable
roles[varnames %in% vars.to.drop] = "x" #variables to exclude from analysis

#produce GUIDE Description File
write("cleanData.csv", file = "GUIDEdesc.txt")
write("NA",file="GUIDEdesc.txt",append=TRUE)
write("2",file="GUIDEdesc.txt",append=TRUE)
write.table(cbind(1:nvar,varnames,roles),file="GUIDEdesc.txt",
row.names=FALSE,col.names=FALSE,quote=FALSE,append=TRUE)
```
Of the remaining 200 variables, 123 variables were found to be unimportant in predicting INTP by GUIDE's importance scoring algorithm. The 20 most important variables can be seen in the following figure. After excluding the unimportant variables, the data set now contains 77 variables that will be used in the subsequent analysis.

```{r Plot Importance Scores, echo=FALSE, cache=TRUE}
par(las=1,mar=c(5,12,4,2),cex=0.75)
leg.col <- c("orange","yellow")
leg.txt <- c("highly important","likely important")
x <- read.table("impScores.txt",header=TRUE)
score <- x$Score
vars <- x$Variable
type <- x$Type
barcol <- rep("orange",length(vars))
barcol[type == "L"] <- "yellow"
barcol[type == "U"] <- "cyan"
n <- sum(x$Type != "U")
barplot(rev(score[1:20]),names.arg=rev(vars[1:20]),
col=rev(barcol[1:20]),horiz=TRUE,xlab="GUIDE importance scores", main = "Top 20 Important Variables")
abline(v=1,col="red",lty=2)
legend("bottomright",legend=leg.txt,fill=leg.col)


```


The top three variables for predicting INTP are PINCP, AGEP, and POVPIP. PINCP gives a persons total income, AGEP is a person's age, and POVPIP is an income to poverty ratio metric. It seems that all of these may be measuring directly or indirectly a person's ability to earn, and thus invest, money. The connection is very clear for PINCP, but is more subtle in AGEP. Typically people do not invest a substantial amount of money before the age of 20, and as their portfolio grows (in dollars and in time), they are more likely to earn money at a faster rate thanks to compound interest. POVPIP is also logically connected to a person's monetary standing, and therefore is unsurprisingly useful in predicting INTP.


```{r Final Data Cleanse, include = FALSE, cache = TRUE}
#extract names of significant variables found in GUIDE to get final version of the data set for analysis
signifVarsOneString = read_lines("cleanDataFinal.txt", skip = 4, n_max = 1)
signif.Vars = strsplit(signifVarsOneString, split = " ")[[1]]
final.data = clean.data.2[signif.Vars]
write_csv(final.data, "finalData.csv") #all unimportant variables removed; 77 remaining
```

## GUIDE Methods

#### Estimation of Mean Using GUIDE and Inverse Probability Weighting
A GUIDE classification tree/forest was used to estimate the probability that an INTP response would be non-missing. Then the Inverse Probability Weighting formula was used to estimate the mean of INTP. These methods are then compared to a niave estimation of the mean, where we simply ignore missing values in INTP, and the simple weighted estimate of the mean.

```{r IPW Mean Estimation, echo = FALSE, cache = TRUE}
#IPW method with classification GUIDE Tree
zclass = read.table("classfit.txt", header = TRUE)
p.nonmissing = zclass[,5] #prob INTP is non-missing
g = !is.na(clean.data.2$INTP) #nonmissing INTP obs
w = clean.data.2$PWGTP
ipw = sum(w[g]*clean.data.2$INTP[g]/p.nonmissing[g])/sum(w[g]/p.nonmissing[g])

#IWP method with classification GUIDE Forest
zclass.forest = read.table("classfitForest.txt", header = TRUE)
p.forest = zclass.forest[,2]
ipw.forest = sum(w[g]*clean.data.2$INTP[g]/p.forest[g])/sum(w[g]/p.forest[g])

naive.mean = mean(clean.data.2$INTP, na.rm = T) #mean from ignoring misisng values
simple = (sum(w[g]*clean.data.2$INTP[g]))/sum(w[g])

results = data.frame("GUIDE Tree Prediction" = ipw, 
                     "GUIDE Forest Prediction" = ipw.forest,
                     "Simple Weighted Sum" = simple,
                     "Niave Mean" = naive.mean)

kable(results, digits = 2,
      col.names = c("GUIDE Tree Prediction", "GUIDE Forest Prediction", "Simple Weighted Sum", "Niave Mean"))


# gt(results) %>% 
#   cols_label("GUIDE.Tree.Prediction" = "GUIDE Tree Prediction",
#              "GUIDE.Forest.Prediction" = "GUIDE Forest Prediction",
#              "Simple.Weighted.Sum" = "Simple Weighted Sum",
#              "Niave.Mean" = "Niave Mean") %>% 
#   tab_header(title = "GUIDE IPW Methods",
#              subtitle = "Figure 1")
```


#### Estimation of Mean Using GUIDE Imputation
```{r Guide Tree and Forest Imputation, echo = FALSE, cache = TRUE}
zreg = read.table("regFit.txt", header = T)
yhat = zreg$predicted
imputed.tree = ((sum(w[g]*clean.data.2$INTP[g])) + (sum(w[!g]*yhat[!g])))/(sum(w))  

zreg.forest = read.table("regForestFit.txt", header = T)
yhat.f =zreg.forest$predicted
imputed.forest = ((sum(w[g]*clean.data.2$INTP[g])) + (sum(w[!g]*yhat.f[!g])))/(sum(w)) 

reg.table = data.frame("tree" = imputed.tree,
                       "forest" = imputed.forest)

kable(reg.table, digits = 3, col.names = c("Tree Estimate", "Forest Estimate"))
```

## Adjustment to Data Set
In the middle of the project, it has come to my attention that there were some very young people surveyed by the ACS. For the remainder of the analysis methods, people younger than 15 years old will be removed from the data set. With these observations removed, there are now 57411 rows and 77 columns. I have chosen not to redo the GUIDE analysis at this time because redoing all the calculations will cost at least 12 hours of cpu runtime, and there is still plenty left to do in the project. I anticipate the guide methods will result in a *lower estimations of the mean* than the following methods will because it includes these younger members. These younger members will most likely have lower values for variables important to predicting INTP such as annual income. With that being said, the GUIDE methods do not produce unreasonably low estimates; however, we will acknowledge this bias when making conclusions about the mean of INTP after other methods have been considered.

```{r data clean for rest of methods, cache = TRUE, include = FALSE}
data = read_csv("psam_p47.csv", guess_max = 10000)
data = data[!is.na(data$INTP),] #removes people younger than 15 from data set

#all flag variables start with an F, but some other columns might be absorbed too
possible.flag.vars = names(data %>% 
  select(starts_with("F"))) 

non.flag.vars = "FOD2P" #variable starting with F that isn't a flag variable
flag.vars = possible.flag.vars[!(possible.flag.vars %in% non.flag.vars)] #removes none-flag vars 
actual.vars = sub('.','',flag.vars) #removes leading F in flag variables
act.vars.in.data.logi = actual.vars %in% names(data) #not all of the real variables exist in the person data set

#update vectors to contain only the variables present in the data set for both the flag variables and the regular variable
flag.vars = flag.vars[act.vars.in.data.logi]
actual.vars = actual.vars[act.vars.in.data.logi]
index = 1 #will be used to index into actual.vars in the for loop 

#eplace value with NA if flag variable is one 
for(var in flag.vars){
flag.var.one = data[,var] == 1
data[,actual.vars[index]][flag.var.one] <-  NA
index = index + 1
}

vars.to.drop = c("ADJINC", "DIVISION", "FANCP", "REGION", "RT", "SERIALNO", "SPORDER", "ST")
clean.data.1 = data %>% 
  select(-all_of(vars.to.drop)) %>% 
  select(-num_range("PWGTP", 1:80))

num.vars = c("PWGTP","INTP", "AGEP", "JWMNP", "OIP","JWRIP","PAP", "OIP", "RETP","SEMP", "SSIP", "SSP", "WKWN", "WKHP", "WAGP","PINCP", "PERNP","POVPIP", "CITWP", "MARHYP", "YOEP") #numerical variables 
varnames.clean = names(clean.data.1) #get column names of clean data frame
cat.vars = varnames.clean[!(varnames.clean %in% num.vars)] #all categorical variables

#this data set has correct column types 
final.data = clean.data.1 %>% 
  mutate(across(all_of(num.vars), as.double)) %>% #changes type to double 
  mutate(across(all_of(cat.vars), as.factor)) #change to factor

signifVarsOneString = read_lines("cleanDataFinal.txt", skip = 4, n_max = 1)
signif.Vars = strsplit(signifVarsOneString, split = " ")[[1]]
signif.Vars[78] = "FINTP" #dont want to remove this
final.data = final.data[signif.Vars] ##57411 by 77
dim.final.data = dim(final.data)
```

## RPART

#### IPW with RPART
The best RPART tree includes the variable ANC1P; however, this variable has greater than 52 levels, and thus is not useful when looking at a plot of the tree. Even changing text arguments will not result in a usable plot. This is also the case for the variable SOCP when ANC1P is removed from the analysis.

In order to give a simple, usable graph for an RPART tree, I have removed these variables *from the diagram only.* Note that I kept the best rpart tree in order to estimate the mean of INTP using the IPW method. The best RPART tree uses FHINS6P, ANC1P, and NWLA as split variables.
```{r rpart, cache = TRUE, echo = FALSE}
temp = final.data %>% 
  select(-ANC1P) %>% 
  select(-SOCP)

rp.tree = rpart(FINTP ~ . - INTP - PWGTP, data = temp, method = "anova") #simple graph
rp.real = rpart(FINTP ~. - INTP - PWGTP, data = final.data, method = "anova") #usef for estimate
rpart.plot(rp.tree, main = "RPART Tree For Estimating Probability of Non-Missing INTP")
p = predict(rp.real)
w = final.data$PWGTP
y = final.data$INTP
gp = !is.na(y)
ipw.rpart =  sum(w[gp]*y[gp]/p[gp])/sum(w[gp]/p[gp])
```

#### RPART Imputation Estimation
```{r rpart imputed, cache = TRUE, echo = FALSE}
temp = final.data %>% 
  select(-ANC1P) %>% 
  select(-FINTP)

rp.imp = rpart(INTP ~ ., weight = PWGTP, data = temp, method = "anova")
rpart.plot(rp.imp, main = "RPART Tree for Estimating INTP")
w = final.data$PWGTP
y = final.data$INTP
miss = is.na(y)
yhat = predict(rp.imp, newdata = temp)
rp.estimate =  (sum(w[!miss]*y[!miss])+sum(w[miss]*yhat[miss]))/sum(w)
```

#### RPART Estimates of INTP
```{r RPART Results, echo=FALSE, cache=FALSE}
x = tibble("RPART IPW" = ipw.rpart,
           "RPART Impute" = rp.estimate)
kable(x)
```

## CTREE
I have removed categorical variables with greater than 31 levels in order to use CTREE. Note that in the regression tree, rows with missing INTP values have been removed as CTREE can not handle missing values in the response variable.
```{r CTREE IPW, echo = FALSE, cache = TRUE}

#if prediction file exists, read the data from that. otherwise run ctree algorithm
if(file.exists("ctreePred.txt")){
  table = read_table("ctreePred.txt", col_types = "i")
  p = table$predicted
}else{
  fmla = formula(FINTP ~. - INTP - PWGTP)
  ct = ctree(fmla, data = final.data.factor) 
  p = as.numeric(predict(ct))
  ctree.results = tibble("predicted" = p)
  write_csv(ctree.results, file = "ctreePred.txt")
}

#plot(ct, type = "simple", drop_terminal = T) plot not useful.

#ipw method 
w = final.data$PWGTP
y = final.data$INTP
gp = !is.na(y)
ipw.ct = sum(w[gp]*y[gp]/p[gp])/sum(w[gp]/p[gp])

```

```{r CTREE Regression, echo = FALSE, cache = T}
final.data.no.missing = final.data[!is.na(final.data$INTP),]

#find variables that have greater than 31 levels
# for(var in names(final.data.no.missing)){
#   if(is.factor(final.data.no.missing[[var]])){
#    if(nlevels(final.data.no.missing[[var]]) > 31){
#      print(var)
#    }
#   }
# }

#variables with greater than 31 levels
high.level.vars = c("ANC1P","FOD1P", "INDP", "NAICSP", "PUMA", "OCCP", "RAC2P", "SOCP")
no.high.level = final.data.no.missing %>% 
  select(-all_of(high.level.vars))



if(file.exists("ctreeRegYHAT.txt")){
  table = read_table("ctreeRegYHAT.txt", col_types = "i")
  yhat = table$yhat
}else{
  ct.reg = ctree(INTP ~. - FINTP, weights = no.high.level$PWGTP, data = no.high.level)
  yhat = predict(ct.reg, newdata = no.high.level)
  ct.reg.results = tibble("yhat" = yhat)
  write_csv(ct.reg.results, file = "ctreeRegYHAT.txt")
}

w = no.high.level$PWGTP
y = no.high.level$INTP
miss = is.na(y)
ct.reg.est = (sum(w[!miss]*y[!miss])+sum(w[miss]*yhat[miss]))/sum(w)
```

```{r ctree results, cache = TRUE, echo = FALSE}
ctree.results = tibble("IPW" = ipw.ct,
                       "Reg" = ct.reg.est)
kable(ctree.results, col.names = c("CTREE IPW Estimate", "CTREE Regression Estimate"), digits = 2)

```

## Imputation with MICE
I originally tried running MICE on the full data set with all variables included; however, there were some problems with this. MICE makes a call to nnet, which will take much longer to run with high level categorical variables included, and sometimes it crashes altogether. I raised the maximum number of weights from 1000 to 5000 in nnet, but it could still not handle some high level variables. I have removed unimportant high level variables with greater than 31 levels in order to run MICE. The variable JWRIP also caused problems within MICE, as it would consistently cause the glm.fit function to fail to converge. I then all variables I removed to run MICE back in and did mode imputation of missing values, as these were all categorical variables.
```{r Missing Value Imputation, echo = FALSE, cache = TRUE}

if(file.exists("miceDataNoNa.csv")){
  mice.data = read_csv("miceDataNoNA.csv")
}else{
  y = no.high.level$INTP 
  no.intp = no.high.level %>% select(-INTP) %>% select(-JWRIP)
  mice.result = mice(no.intp, m = 1, nnet.MaxNWts = 5000)
  mice.data = complete(mice.result,1)
  mice.copy = mice.data
  
  #add back variables that were taken out to run mice
  for(var in high.level.vars){
    mice.copy[var] = final.data.no.missing[[var]]
  }
  
  mice.copy$INTP = final.data.no.missing$INTP
  mice.copy$JWRIP = final.data.no.missing$JWRIP
  
  #impute with mode
  mice.copy = impute(mice.copy)
  colSums(is.na(mice.copy))
  write_csv(mice.copy, file = "miceDataNoNA.csv")
  
}


```


```{r mode imputation, cache = TRUE, echo = FALSE}



```


```{r}


```

```{r Logisitic Regression}

```


```{r randomForest Regression}

```

```{r CFOREST Regression}

```

### stuff do to at the end
add in latex formula for ipw and imputed method 
make sure all graphs have titels and labels 
organize project in most logical way
